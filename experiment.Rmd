---
title: "Transparency vs Performances"
output:
  html_document:
    toc: true
    code_folding: "hide"
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
library(ggplot2)
library(RColorBrewer)
cbPalette <- rev(brewer.pal(8,"RdYlGn"))
```

# Motivations

In these experiments we aimed to observe
variations of performances
of different multi-label classification 
algorithms, 
according to their degree of transparency.

These experiments was made using
the Mulan Java Library, developped by @tsoumakas2011mulan,
which contains plethora of classification algorithms,
but also tools to evaluate them.

# Classification systems

## BPMLL

Implementation of Back-Propagation Multi-Label Learning,
based on the work of @zhang2006.

BPMLL is based on neural network.

According to our criteria of "transparency", BPMLL is the less "transparent"
classification system evaluated.

## MLkNN

Implementation of the Multi-Label k-Nearest Neighbours
algorith, based on the work of @zhang2007.

MLkNN is based on k-Nearest Neighbours clustering algorithm and
co-occurences probabilities.

According to our criteria of "transparency", MLkNN is the third less
"transparent" classification system evaluated.

## RAkEL

Implementation of the RAndom k-labELsets alogrithm,
based on the work of @tsoumakas2011rakel.

RAkEL is a multilabel classifier using other classifier
designed for simple classification problem.

### C4.5

Implementation of the C4.5 algorithm, based on the
work of @quinlan1993.

C4.5 is based on decision trees and shannon's entropy.

According to our criteria of "transparency", C4.5 is the third
most "transparent" classification system evaluated.

### Naive Bayes

Implementation of a Naive Bayes classification algorithm,
based on the work of @john1995.

Naive Bayes is based on probalities and the bayes theorem.

According to our criteria of "transparency", Naive Bayes 
with RAkEL is the second most "transparent" classification system evaluated.

### RIPPER

Implementation of Repeated Incremental Pruning 
to Produce Error Reduction algorithm,
based on the work of @cohen1995.

RIPPERÂ is based on rule-based classifier and shannon's entropy.

According to our criteria of "transparency", RIPPER is
the fourth most "transparent" classification system evaluated.

### SMO

Implementation of sequential minimal optimization 
algorithm for training a support vector classifier,
based on the work of @platt1998, @hastie1998
and @keerthi2001.

SMO is based on the aggregation of several mathemical functions.

According to our criteria of "transparency", SMO is
the second less "transparent" classification system evaluated.

## HistBayes

A version of the Naive Bayes algorithm, proposed for this study,
which discretize numerical variables using histograms.
Apply the Naive Bayes algorithm for each label independently.

HistBayes is based on histograms, probabilities and the bayes theorem.

According to our criteria of "transparency", HistBayes is the most "transparent"
classification system evaluated.

## FuzzyBayes

A version of the Naive Bayes algorithm, proposed for this study,
which discretize numerical variables using the fuzzy c-means
clustering algorithm proposed by @bezdek1984.

FuzzyBayes is based on fuzzy sets, probabilities and the bayes theorem.

According to our criteria of "transparency", FuzzyBayes is the fifth 
most "transparent" classification system evaluated.

# Datasets

Here are listed datasets used for this test plan.

According to @tsoumakas2007, 
a dataset $D$ is defined as a set of instances,
a set of attributes $X$ (nominal or numeric)
and a set of labels $L$ (with 0 or 1 as value).

A dataset consisting of $|D|$ multi-instances
$(x_i, Y_i), i = 1..|D|$, 
with $Y_i \subseteq L$ the set of labels equals
to 1 for the instance $i$.

Then, common attributes of a dataset are :

- number of instances $|D|$
- number of attributes $|X|$
- number of labels $|L|$
- label cardinality $LC(D)$
$$LC(D) = \frac{1}{|D|}\sum_{i=1}^{|D|}|Y_i|$$
- label density $LD(D)$
$$LD(D) = \frac{1}{|D|}\sum_{i=1}^{|D|}\frac{|Y_i|}{|L|}$$


## Birds

Proposed by @briggs2013.

Statistics :

- instances : 645
- attributes :
    - nominal : 2
    - numeric : 258
- labels : 19
- cardinality : 1.014
- density : 0.053

## CAL500

Proposed by @turnbull2008.

Statistics :

- instances : 502
- attributes :
    - nominal : 0
    - numeric : 68
- labels : 174
- cardinality : 26.004
- density : 0.149 

## Consultations

Proposed for this experiment.

Statistics:

- instances : 50
- attributes :
    - nominal : 2
    - numeric : 2
- labels : 18
- cardinality : 5.4
- density : 0.3

## Emotions

Proposed by @trohidis2008.

Statistics :

- instances : 593
- attributes :
    - nominal : 0
    - numeric : 72
- labels : 6
- cardinality : 1.869
- density : 0.312 

## Genbase

Proposed by @diplaris2005.

Statistics :

- instances : 662
- attributes :
    - nominal : 1186
    - numeric : 0
- labels : 27
- cardinality : 1.252
- density : 0.046 

## Medical

Proposed by @pestian2007.

Statistics :

- instances : 978
- attributes :
    - nominal : 1449
    - numeric : 0
- labels : 45
- cardinality : 1.245
- density : 0.028 

## Scene

Proposed by @boutell2004.

Statistics :

- instances : 2407
- attributes :
    - nominal : 0
    - numeric : 294
- labels : 6
- cardinality : 2.402
- density : 0.4 


# Performances

```{r loadCsv}
results <- read.csv("results/crossvalidation.csv", sep=";")
lessTransparent2MostTransparent <- c("BPMLL","RAkEL+SMO","MLkNN","FuzzyBayes","RAkEL+Ripper","RAkEL+C4.5","RAkEL+NaiveBayes","HistBayes")
mostTransparent2lessTransparent <- rev(lessTransparent2MostTransparent)
```

## Hamming Loss

According to @schapire2000, Hamming Loss of a
classifier $H$ on a dataset $D$ is defined as follow :

$$HammingLoss(H,D) = \frac{1}{|D|}\sum_{i=1}^{|D|}\frac{Y_i \Delta Z_i}{|L|}$$

Where $\Delta$ is corresponding to symmetric difference of
two sets of labels (equivalent to XOR operator).

The hamming loss is corresponding to the mean of errors by labels.

```{r hammingLoss}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Hamming_Loss,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Hamming_Loss - Hamming_Loss_std,
    ymax=Hamming_Loss + Hamming_Loss_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
ylab("Hamming Loss") +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ggtitle("Hamming losses of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r hammingLossBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Hamming_Loss
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_dodge(-0.9), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
labs(color="Classification Systems")+
scale_colour_manual(values = cbPalette)+
ylim(0.0,1.0) +
ylab("Hamming Loss") +
ggtitle("Distribution of hamming losses of multi-label classification systems by datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r hammingLossLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Hamming_Loss
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
xlab("Learner") +
ylim(0.0,1.0) +
ylab("Hamming Loss") +
ggtitle("Distribution of hamming losses by multi-label classification systems for different datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

## Precision

The precision rate is a well-known criterion to evaluate the performance of a classifier $H$ on a dataset $D$.

For one-label classification problems the precision rate is defined as follow :

$$Prec(H,D,l) = \frac{TPs(H,D,l)}{TPs(H,D,l) + FPs(H,D,l)}$$

With $l \in L$ a label, $TPs$ the number of true positives of the classifier $H$ on the dataset $D$ for the label $l$
and $FPs$ the number of false positives of the classifier $H$ on the dataset $D$ for the label $l$.

The precision rate giving us the percentage of good labelisation over all labelisation made by the classifier (aka l=1).

For multi-label classification problems the precision rate is generally averaged to have an overview of the precision
of the classifier $H$ for all labels.

### Micro-averaged

First, micro-averaged precision $Prec^{micro}:H \times D \rightarrow [0,1]$ which compute the precision
of all labelisation of $H$ regardless of the label.

Micro-averaged precision of a classifier $H$ on a dataset $D$ is defined as follow:

$$Prec^{micro}(H,D) = \frac{\sum_{l \in L} TPs(H,D,l)}{\sum_{l \in L}(TPs(H,D,l) + FPs(H,D,l))}$$


```{r microPrecision}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Micro.averaged_Precision,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Micro.averaged_Precision - Micro.averaged_Precision_std,
    ymax=Micro.averaged_Precision + Micro.averaged_Precision_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
ylab("Micro-averaged Precision") +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ggtitle("Micro-averaged precisions of multi-label classification systems by datasets") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r microPrecisionBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Micro.averaged_Precision
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_dodge(-0.9), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
labs(color="Classification Systems")+
scale_colour_manual(values = cbPalette)+
ylim(0.0,1.0) +
ylab("Micro-averaged Precision") +
ggtitle("Distribution of micro-averaged precisions of multi-label classification systems by datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r microPrecisionLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Micro.averaged_Precision
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
xlab("Learner") +
ylim(0.0,1.0) +
ylab("Micro-averaged Precision") +
ggtitle("Distribution of micro-averaged precisions by multi-label classification systems for different datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

### Macro-averaged

Secondly, the macro-averaged precision $Prec^{macro} : H \times D \rightarrow [0,1]$, which compute the 
mean of precision of a classifier $H$ on a dataset $D$, is defined as follow:

$$Prec^{macro}(H,D) = \frac{\sum_{l \in L} Prec(H,D,l)}{|L|}$$

```{r macroPrecision}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Macro.averaged_Precision,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Macro.averaged_Precision - Macro.averaged_Precision_std,
    ymax=Macro.averaged_Precision + Macro.averaged_Precision_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
ylab("Macro-averaged Precision") +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ggtitle("Macro-averaged precisions of multi-label classification systems by datasets") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r macroPrecisionBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Macro.averaged_Precision
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_dodge(-0.9), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
labs(color="Classification Systems")+
scale_colour_manual(values = cbPalette)+
ylim(0.0,1.0) +
ylab("Macro-averaged Precision") +
ggtitle("Distribution of macro-averaged precisions of multi-label classification systems by datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r macroPrecisionLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Macro.averaged_Precision
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
xlab("Learner") +
ylim(0.0,1.0) +
ylab("Macro-averaged Precision") +
ggtitle("Distribution of macro-averaged precisions by multi-label classification systems for different datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

## Recall

The precision rate is generally associated to another performance rate: the recall, noted 
$Recall: H \times D \times l \rightarrow [0,1]$..

For one-label classification problems, the recall of a classifier $H$ on a dataset $D$ is
defined as follow:

$$Recall(H,D,l) = \frac{TPs(H,D,l)}{TPs(H,D,l) + FNs(H,D,l)}$$

With $l \in L$ a label, $TPs$ the number of true positives of classifier $H$ on a dataset $D$ for a label $l$ 
and $FNs$ the number of false negatives of classifier $H$ on a dataset $D$ for a label $l$.

### Micro-averaged

The micro-averaged recall of a classifier $H$ on a dataset $D$ is defined as follow:

$$Recall^{micro}(H,D) = \frac{\sum_{l \in L} TPs(H,D,l)}{\sum_{l \in L}(TPs(H,D,l) + FNs(H,D,l))}$$


```{r microRecall}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Micro.averaged_Recall,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Micro.averaged_Recall - Micro.averaged_Recall_std,
    ymax=Micro.averaged_Recall + Micro.averaged_Recall_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
ylab("Micro-averaged Recall") +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ggtitle("Micro-averaged recalls of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r microRecallBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Micro.averaged_Recall
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_dodge(-0.9), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
labs(color="Classification Systems")+
scale_colour_manual(values = cbPalette)+
ylim(0.0,1.0) +
ylab("Micro-averaged Recall") +
ggtitle("Distribution of micro-averaged recalls of multi-label classification systems by datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r microRecallLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Micro.averaged_Recall
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
xlab("Learner") +
ylim(0.0,1.0) +
ylab("Micro-averaged Recall") +
ggtitle("Distribution of micro-averaged recalls by multi-label classification systems for different datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

### Macro-averaged

The macro-averaged recall of a classifier $H$ on a dataset $D$ is defined as follow:

$$Recall^{macro}(H,D) = \frac{\sum_{l \in L} Recall(H,D,l)}{|L|}$$

```{r macroRecall}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Macro.averaged_Recall,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Macro.averaged_Recall - Macro.averaged_Recall_std,
    ymax=Macro.averaged_Recall + Macro.averaged_Recall_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ylab("Macro-averaged Recall") +
ggtitle("Macro-averaged recalls of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r macroRecallBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Macro.averaged_Recall
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_dodge(-0.9), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
ylim(0.0,1.0) +
labs(color="Classification Systems")+
scale_colour_manual(values = cbPalette)+
ylab("Macro-averaged Recall") +
ggtitle("Distribution of macro-averaged recalls of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r macroRecallLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Macro.averaged_Recall
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
ylim(0.0,1.0) +
xlab("Learner") +
ylab("Macro-averaged Recall") +
ggtitle("Distribution of macro-averaged recalls by multi-label classification systems for different datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```


## Specificity

The recall/sensibility rate is also generally associated to another performance rate: the specificity, noted 
$Spec: H \times D \times l \rightarrow [0,1]$

For one-label classification problems, the specificity of a classifier $H$ on a dataset $D$ is
defined as follow:

$$Spec(H,D,l) = \frac{TNs(H,D,l)}{TNs(H,D,l) + FPs(H,D,l)}$$

With $l \in L$ a label, $TNs$ the number of true negatives of classifier $H$ on a dataset $D$ for a label $l$ 
and $FPs$ the number of false positives of classifier $H$ on a dataset $D$ for a label $l$.

### Micro-averaged

The micro-averaged specificity of a classifier $H$ on a dataset $D$ is defined as follow:

$$Spec^{micro}(H,D) = \frac{\sum_{l \in L} TNs(H,D,l)}{\sum_{l \in L}(TNs(H,D,l) + FPs(H,D,l))}$$


```{r microSpec}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Micro.averaged_Specificity,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Micro.averaged_Specificity - Micro.averaged_Specificity_std,
    ymax=Micro.averaged_Specificity + Micro.averaged_Specificity_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
ylab("Micro-averaged Specificity") +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ggtitle("Micro-averaged specificity of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r microSpecBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Micro.averaged_Specificity
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_dodge(-0.9), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
labs(color="Classification Systems")+
scale_colour_manual(values = cbPalette)+
ylim(0.0,1.0) +
ylab("Micro-averaged Specificity") +
ggtitle("Distribution of micro-averaged specificity of multi-label classification systems by datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r microSpecLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Micro.averaged_Specificity
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
xlab("Learner") +
ylim(0.0,1.0) +
ylab("Micro-averaged Specificity") +
ggtitle("Distribution of micro-averaged specificity by multi-label classification systems for different datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

### Macro-averaged

The macro-averaged specificity of a classifier $H$ on a dataset $D$ is defined as follow:

$$Spec^{macro}(H,D) = \frac{\sum_{l \in L} Spec(H,D,l)}{|L|}$$

```{r macroSpec}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Macro.averaged_Specificity,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Macro.averaged_Specificity - Macro.averaged_Specificity_std,
    ymax=Macro.averaged_Specificity + Macro.averaged_Specificity_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ylab("Macro-averaged Specificity") +
ggtitle("Macro-averaged specificity of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r macroSpecBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Macro.averaged_Specificity
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_dodge(-0.9), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
ylim(0.0,1.0) +
labs(color="Classification Systems")+
scale_colour_manual(values = cbPalette)+
ylab("Macro-averaged Specificity") +
ggtitle("Distribution of macro-averaged specificity of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r macroSpecLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Macro.averaged_Recall
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
ylim(0.0,1.0) +
xlab("Learner") +
ylab("Macro-averaged Specificity") +
ggtitle("Distribution of macro-averaged specificity by multi-label classification systems for different datasets") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

## F-Measure

Lastly, to have an overview the performances of a classifier a harmonic mean of precision and recall,
also called F-measure, is made.

For one-label classification problems, the F-measure of a classifier $H$ on a label $l$
of a dataset $D$ is defined as follow:

$$F(H,D,l) = \frac{Prec(H,D,l) \times Recall(H,D,l)}{Prec(H,D,l) + Recall(H,D,l)}$$

For multi-label classification problems, the F-measure is computed for micro and macro averaged rates.

### Micro-averaged

The micro-averaged F-measure of a classifier $H$ on a dataset $D$ is defined as follow:

$$F^{micro}(H,D) = \frac{Prec^{micro}(H,D) \times Recall^{micro}(H,D)}{Prec^{micro}(H,D) + Recall^{micro}(H,D)}$$

```{r microFMeasure}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Micro.averaged_F.Measure,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Micro.averaged_F.Measure - Micro.averaged_F.Measure_std,
    ymax=Micro.averaged_F.Measure + Micro.averaged_F.Measure_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ylab("Micro-averaged F-Measure") +
ggtitle("Micro-averaged F-Measure of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r microFBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Micro.averaged_F.Measure
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
ylim(0.0,1.0) +
labs(color="Learner")+
scale_colour_manual(values = cbPalette)+
ylab("Micro-averaged F-Measure") +
ggtitle("Distribution of micro-averaged F-measures of different multi-label classification systems by dataset") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

```{r microFLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Micro.averaged_F.Measure
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
ylim(0.0,1.0) +
xlab("Learner") +
ylab("Micro-averaged F-Measure") +
ggtitle("Distribution of micro-averaged F-measures by multi-label classification systems for different dataset") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

### Macro-averaged

The macro-averaged F-measure of a classifier $H$ on a dataset $D$ is defined as follow:

$$F^{macro}(H,D) = \frac{Prec^{macro}(H,D) \times Recall^{macro}(H,D)}{Prec^{macro}(H,D) + Recall^{macro}(H,D)}$$

```{r macroFMeasure}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Macro.averaged_F.Measure,
    fill=factor(Learner,levels = mostTransparent2lessTransparent)
  )
) + 
geom_bar(
  stat = "identity",
  color = "black",
  position = position_dodge()
) +
geom_errorbar(
  aes(
    ymin=Macro.averaged_F.Measure - Macro.averaged_F.Measure_std,
    ymax=Macro.averaged_F.Measure + Macro.averaged_F.Measure_std
  ),
  width=.2,
  position = position_dodge(.9)
) +
ylim(0.0,1.0) +
ylab("Macro-averaged F-Measure") +
labs(fill="Learner")+
scale_fill_manual(values = cbPalette)+
ggtitle("Macro-averaged F-Measure of multi-label classification systems by dataset") +
theme(plot.title = element_text(size=10, face="bold", hjust=0.5))
```

```{r macroFBox}
ggplot(
  results,
  aes(
    x=Dataset,
    y=Macro.averaged_F.Measure
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=factor(Learner,levels = mostTransparent2lessTransparent))) +
ylim(0.0,1.0) +
ylab("Macro-averaged F-Measure") +
labs(color="Learner")+
scale_colour_manual(values = cbPalette)+
ggtitle("Distribution of macro-averaged F-measures of different multi-label classification systems by dataset") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

```{r macroFLearnerBox}
ggplot(
  results,
  aes(
    x=factor(Learner,levels = lessTransparent2MostTransparent),
    y=Macro.averaged_F.Measure
  )
) + 
geom_boxplot() +
geom_jitter(shape=15, size=2, position=position_jitter(0.2), aes(colour=Dataset)) +
ylim(0.0,1.0) +
ylab("Macro-averaged F-Measure") +
xlab("Learner") + 
ggtitle("Distribution of macro-averaged F-measures by multi-label classification systems for different dataset") +
theme(plot.title = element_text(size=9, face="bold", hjust=0.5))
```

# References

